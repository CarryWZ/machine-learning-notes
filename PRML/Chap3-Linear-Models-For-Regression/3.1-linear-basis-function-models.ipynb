{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 线性基函数模型\n",
    "\n",
    "本章开始，我们将学习监督学习中的一个重要任务——回归。给定一个$D$维的输入向量$x\\in \\mathbb{R}^D$，回归的目标是预测与之对应的目标变量$t\\in\\mathbb{R}$。对于回归，我们并不陌生，因为早在第一章的[多项式曲线拟合](../Chap1-Introduction/1.1-polynomial-curve-fitting.ipynb)一节中，我们就已经遇到了一个回归问题：\n",
    "$$y(x,\\mathbf{w})=\\sum_{j=0}^Mw_jx^j$$\n",
    "我们使用了上式中的多项式函数来拟合一个未知函数在各个点的目标函数值。事实上，多项式函数是本节将要介绍的线性回归模型的一个特例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归模型\n",
    "\n",
    "我们先来看一个最简单的线性回归模型\n",
    "\n",
    "$$ y(\\mathbf{x},\\mathbf{w})=w_0 + w_1 x_1 + ... + x_D w_D $$\n",
    "\n",
    "该模型中，拟合函数不仅是关于参数的线性函数，同时也是关于输入变量的线性函数。我们把这类关于参数呈线性的模型称为`线性回归模型`（linear regression model），该模型对应于多项式函数在$M=1$时的情形。  \n",
    "\n",
    "回顾第一章的[多项式曲线拟合](../Chap1-Introduction/1.1-polynomial-curve-fitting.ipynb)中进行的实验，当$M=1$时，多项式函数的拟合效果很差。\n",
    "\n",
    "为了提高线性模型的拟合能力，我们引入基函数（basis function）对输入变量作一个非线性变换$\\phi(\\mathbf{x})$，于是引入了基函数的线性回归模型可以表示为\n",
    "$$y(\\mathbf{x},\\mathbf{w})=w_0 + \\sum_{j=1}^{D-1} w_j \\phi(x_j)$$\n",
    "为了简化上式，我们定义$\\phi(x_0)=1$，于是上式又可以简写为\n",
    "$$y(\\mathbf{x},\\mathbf{w})=\\sum_{j=0}^{D-1} w_j \\phi(x_j)=\\mathbf{w}^\\top \\phi(\\mathbf{x})$$\n",
    "其中，$\\mathbf{w}=(w_0,w_1,...,w_{D-1})^\\top$,$\\phi(\\mathbf{x})=(\\phi_0(\\mathbf{x}),\\phi_1(\\mathbf{x}),...,\\phi_{D-1}(\\mathbf{x}))^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可供我们选择的基函数有：\n",
    "\n",
    "* 高斯基函数\n",
    "\n",
    "$$ \\phi_j(x)=\\exp\\big\\{-\\frac{(x-\\mu_j)^2}{2s^2}\\big\\}$$\n",
    "\n",
    "* S基函数\n",
    "$$ \\phi_j(x)=\\sigma\\big(-\\frac{x-\\mu_j}{s}\\big)$$\n",
    "\n",
    "其中$\\sigma(a)=\\frac{1}{1+\\exp(-a)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大似然和最小二乘\n",
    "\n",
    "回顾第一章，我们通过最小化平方和误差求出了最优多项式曲线的参数，并且证明了在高斯噪声的假设下，平方和误差函数可以由最大似然法导出。这一节，我们将更深入讨论最大似然和最小二乘之间的关系。\n",
    "\n",
    "像往常一样，我们假设目标变量等于一个确定函数与一个高斯噪声之和\n",
    "\n",
    "$$ t =  y(\\mathbf{x},\\mathbf{w}) + \\epsilon $$\n",
    "\n",
    "其中$\\epsilon$服从均值为0，精度为$\\beta$的高斯分布，即$\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})$。\n",
    "\n",
    "于是$t$也服从高斯分布：\n",
    "\n",
    "$$ t\\sim \\mathcal{N}(y(\\mathbf{x},\\mathbf{w}), \\beta^{-1})) $$\n",
    "\n",
    "对于给定的新样本$\\mathbf{x}^*$，其目标变量最优的预测是\n",
    "\n",
    "$$ t^* = \\int t p(t|\\mathbf{x}^*, \\mathbf{w}, \\beta) dt=y(\\mathbf{x}^*,\\mathbf{w})$$\n",
    "\n",
    "\n",
    "现假设我们从该高斯分布独立同分布地采样得到一个样本集$\\{(\\mathbf{x}_1,t_1), ..., (\\mathbf{x}_N, t_N)\\}$，并令$\\mathrm{t} =(t_1,...,t_N)^\\top$表示由该样本集中所有的目标变量构成的向量。\n",
    "\n",
    "那么$\\mathrm{t}$的联合概率分布就可以表示为\n",
    "\n",
    "$$p(\\mathrm{t}|X,\\mathbf{w},\\beta)=\\prod_{i=1}^N p(t_i|\\mathbf{x}_i,\\mathbf{w},\\beta^{-1})=\\prod_{i=1}^N \\mathcal{N}(t_i|\\mathbf{w}^\\top \\phi(\\mathbf{x}_i), \\beta^{-1} )$$\n",
    "\n",
    "由于$X$已知，可以把$p(\\mathrm{t}|X,\\mathbf{w},\\beta)$简记为$p(\\mathrm{t}|\\mathbf{w},\\beta)$， 对数似然函数为\n",
    "\n",
    "$$ \\begin{aligned}\\ln p(\\mathrm{t}|\\mathbf{w},\\beta) &= \\sum_{i=1}^N \\ln \\mathcal{N}(t_i|\\mathbf{w}^\\top \\phi(\\mathbf{x}_i), \\beta^{-1} ) =\\sum_{i=1}^N\\ln \\sqrt{\\frac{\\beta}{2\\pi}}\\exp\\big\\{-\\frac{\\beta}{2}[t-\\mathbf{w}^\\top \\phi(x_i)]^2\\big\\}\\\\&=\\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln(2\\pi) - \\frac{\\beta}{2} \\sum_{i=1}^N[t_i-\\mathbf{w}^\\top\\phi(x_i)]^2\\\\&=\\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln(2\\pi) - \\beta E_D(\\mathbf{w})\\end{aligned}$$\n",
    "其中\n",
    "$$ E_D(\\mathbf{w})= \\frac{1}{2}\\sum_{i=1}^N[t_i-\\mathbf{w}^\\top\\phi(x_i)]^2$$\n",
    "\n",
    "即是最小二乘法的目标函数。  \n",
    "\n",
    "最大似然方法通过最大化似然函数来找到参数的最优值。在线性回归问题中，我们只关心$\\mathbf{w}$，即找到最大化$\\ln p(\\mathrm{t}|\\mathbf{w},\\beta)$的$\\mathbf{w}$，这等价于最小化最小二乘的目标函数$E_D(\\mathbf{w})$\n",
    "\n",
    "由$E_D(\\mathbf{w})$关于$\\mathbf{w}$的梯度为0\n",
    "$$\\begin{aligned}\\nabla_{\\mathbf{w}} E_D(\\mathbf{w}) &= \\frac{1}{2} \\frac{\\partial \\sum_{i=1}^N(t_i^2-2\\mathbf{w}^\\top \\phi(x_i)t_i+(\\mathbf{w}^\\top\\phi(x_i))^2)}{\\partial  \\mathbf{w}}\\\\&= \\sum_{i=1}^N [\\phi(x_i)\\phi(x_i)^\\top - \\phi(x_i)t_i]=0\\end{aligned}$$\n",
    "得\n",
    "$$ \\sum_{i=1}^N \\phi(x_i)\\phi(x_i)^\\top \\mathbf{w}=\\sum_{i=1}^N\\phi(x_i)t_i $$\n",
    "\n",
    "$$ \\mathbf{w}_{ML} = \\big(\\sum_{i=1}^N \\phi(x_i)\\phi(x_i)^\\top\\big)^{-1} \\sum_{i=1}^N\\phi(x_i)t_i  = (\\Phi^\\top \\Phi)^{-1}\\Phi^\\top \\mathrm{t}$$\n",
    "\n",
    "这个结果称为正规方程（Normal Equation），其中$\\Phi$是一个$N\\times D$的矩阵，一般称之为设计矩阵（Design Matrix）\n",
    "\n",
    "$$ \\Phi=\\begin{bmatrix}\\phi_0(x_0) \\quad \\phi_1(x_0) \\quad... \\quad\\phi_{D-1}(x_0)\\\\\\phi_0(x_1)\\quad \\phi_1(x_1) \\quad... \\quad\\phi_{D-1}(x_1)\\\\\\vdots\\\\\\phi_0(x_N) \\quad \\phi_1(x_N) \\quad... \\quad\\phi_{D-1}(x_N)\\end{bmatrix}$$\n",
    "\n",
    "$ \\Phi^\\dagger=(\\Phi^\\top \\Phi)^{-1}\\Phi^\\top$称为伪逆矩阵（Moore-Penrose pseudo-inverse）,它是逆矩阵概念在非方阵上的推广。注意到，当$\\Phi$是一个方阵时，它的伪逆矩阵等于它的逆矩阵$\\Phi^\\dagger = \\Phi^{-1} (\\Phi^\\top)^{-1} \\Phi^\\top = \\Phi^{-1} \\Phi^{-1} \\Phi = \\Phi^{-1}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最小二乘的几何解释\n",
    "\n",
    "如下图所示，$\\mathrm{t}=(t_1,...,t_N)^\\top$是由所有样本点的目标变量组成的N维向量，$\\varphi_j$表示设施矩阵$\\Phi$的第$j$列，$\\mathcal{S}$表示一个由$\\varphi_j(j=1,2,...,D)$张成的子空间（即$\\Phi$的列空间）\n",
    "\n",
    "<img src=\"http://7xikew.com1.z0.glb.clouddn.com/PRML-3.1-1.png\" width=600px>\n",
    "\n",
    "$\\mathbf{y}$是一个$N$维向量，它的第$j$个位置为$y(\\mathbf{x}_j, \\mathbf{w})$，注意到\n",
    "\n",
    "$$\\begin{aligned}\\mathbf{y}&=\\big(y(\\mathbf{x}_1, \\mathbf{w}),...,y(\\mathbf{x}_N, \\mathbf{w})\\big)^\\top\\\\&=\\big(\\phi(\\mathbf{x}_1)^\\top \\mathbf{w},..., \\phi(\\mathbf{x}_N)^\\top \\mathbf{w}\\big)\\\\&=\\Phi \\mathbf{w}=\\sum_{j=1}^D \\varphi_j w_j\\end{aligned}$$\n",
    "\n",
    "由此可知，$\\mathbf{y}$是$\\varphi_j$的线性组合，因此$\\mathbf{y}$也在子空间$\\mathcal{S}$中  \n",
    "\n",
    "接下来我们来考察最小二乘的目标函数\n",
    "\n",
    "$$ E_D(\\mathbf{w})= \\frac{1}{2}\\sum_{i=1}^N[t_i-\\mathbf{w}^\\top\\phi(x_i)]^2= \\frac{1}{2}\\left\\|\\mathbf{t}-\\mathbf{y}\\right\\|^2$$\n",
    "\n",
    "于是最小二乘问题转化为了在$\\Phi$的列空间中找到一个与$\\mathbf{t}$欧式距离最小的点$\\mathbf{y}$。显然，当$\\mathbf{y}$是$\\mathbf{t}$在$\\Phi$中的正交投影时，欧式距离最小，此时对应的参数值$\\mathbf{w}$刚好等于最大似然估计$\\mathbf{w}_{ML}$。\n",
    "\n",
    "我们来证明这个结论。将$\\mathbf{w}_{ML}$代入$\\mathbf{y}$：\n",
    "\n",
    "$$\\mathbf{y}_1 = (\\mathbf{w}_{ML}^\\top \\phi(\\mathbf{x}_1), ..., \\mathbf{w}_{ML}^\\top \\phi(\\mathbf{x}_N))^\\top = \\Phi \\mathbf{w}_{ML} =\\Phi (\\Phi^\\top \\Phi)^{-1}\\Phi^\\top \\mathbf{t}$$\n",
    "\n",
    "另一方面，设\n",
    "\n",
    "$$\\mathbf{y}_2 = P\\mathbf{t}$$\n",
    "\n",
    "是$\\mathbf{t}$在$\\Phi$的列空间中的投影，其中$P$是投影矩阵\n",
    "\n",
    "\n",
    "\n",
    "我们可以将$\\mathbf{y}_2$表示为以$\\Phi$矩阵中各列为基的线性组合：\n",
    "\n",
    "$$ \\mathbf{y}_2 = \\Phi \\xi $$\n",
    "\n",
    "其中$\\xi$是$N$维的向量，由各个基的系数组成\n",
    "\n",
    "\n",
    "二者的残差向量为\n",
    "\n",
    "$$ \\mathbf{e} = \\mathbf{t} - \\mathbf{y}_2  $$\n",
    "\n",
    "显然，$\\mathbf{e}$是$\\Phi$的列空间的法向量，因此有\n",
    "\n",
    "$$ \\Phi^\\top \\mathbf{e} = \\Phi^\\top (\\mathbf{t} - \\mathbf{y})=\\Phi^\\top (\\mathbf{t}-\\Phi \\xi)=0$$\n",
    "则\n",
    "$$\\Phi^\\top\\Phi \\xi = \\Phi^\\top \\mathbf{t}$$\n",
    "\n",
    "因此\n",
    "\n",
    "$$  \\xi = (\\Phi^\\top\\Phi)^{-1}\\Phi^\\top \\mathbf{t}$$\n",
    "\n",
    "由\n",
    "\n",
    "$$Pt = \\Phi\\xi=\\Phi(\\Phi^\\top\\Phi)^{-1}\\Phi^\\top \\mathbf{t}$$\n",
    "\n",
    "可知\n",
    "\n",
    "$$P=\\Phi(\\Phi^\\top\\Phi)^{-1}\\Phi^\\top$$\n",
    "\n",
    "代入$\\mathbf{y}_2=P\\mathbf{t}$可得\n",
    "\n",
    "$$\\mathbf{y}_2=P\\mathbf{t} = \\Phi(\\Phi^\\top\\Phi)^{-1}\\Phi^\\top\\mathbf{t} = \\Phi \\mathbf{w}_{ML}=\\mathbf{y}_1$$\n",
    "\n",
    "由此可知，最小二乘法求解参数的过程等同于求目标变量$\\mathbf{t}$在设计矩阵$\\Phi$的列空间中的正交投影。值得注意的是，当任意两个$\\varphi_j$之间近似地处于同一个方向时，$\\Phi^\\top\\Phi$接近奇异，可能会导致$\\mathbf{w}$数值不稳定，一般的方法是给$\\Phi^\\top\\Phi$添加一个正则项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化最小二乘\n",
    "\n",
    "回顾1.1节多项式曲线拟合中，我们通过向损失函数引入一个正则项的方法来防止过拟合\n",
    "\n",
    "$$ E_D(\\mathbf{w})+\\lambda E_W(\\mathbf{w}) $$\n",
    "\n",
    "其中$\\lambda $是正则系数，用于控制过拟合的代价，$\\lambda$越大，过拟合的代价越大，因此越不容易发生过拟合；$\\lambda$越小，发生过拟合的风险越大。\n",
    "\n",
    "可以选择的正则项有很多种，其中最简单的一种就是L2正则项（在机器学习中，也叫权重衰减），即参数的平方和：\n",
    "$$ E_W(\\mathbf{w})=\\frac{1}{2}\\mathbf{w}^\\top \\mathbf{w}$$\n",
    "\n",
    "引入了L2正则项的损失函数变为\n",
    "\n",
    "$$ \\frac{1}{2}\\sum_{i=1}^N[t_i-\\mathbf{w}^\\top\\phi(x_i)]^2 + \\frac{\\lambda}{2}\\mathbf{w}^\\top \\mathbf{w}$$\n",
    "\n",
    "此时正规方程将变为\n",
    "\n",
    "$$ \\mathbf{w}_{ML}=(\\Phi^\\top \\Phi + \\lambda I)^{-1}\\Phi^\\top \\mathrm{t}$$\n",
    "\n",
    "不失一般性，我们可以用一个更通用的形式来表示正则化目标函数\n",
    "\n",
    "$$ \\frac{1}{2}\\sum_{i=1}^N[t_i-\\mathbf{w}^\\top\\phi(x_i)]^2 + \\frac{\\lambda}{2} \\sum_{j=1}^D |w_j|^q $$\n",
    "\n",
    "下图展示了当$q$的取不同值时，正则项的等高线变化\n",
    "![](http://7xikew.com1.z0.glb.clouddn.com/PRML-3.1-2.png)\n",
    "\n",
    "L2正则项对应于$q=2$的情况。$q=1$时（L1正则项）在统计学中一般被称为Lasso回归，它可以使权值向量中绝大多数的$w_j$都为0，只有少量权值大于0。为了更直观地理解这一点，我们可以将损失函数改写为一个带约束最优化问题\n",
    "\n",
    "$$\\begin{aligned} &\\min \\frac{1}{2}\\sum_{i=1}^N[t_i-\\mathbf{w}^\\top\\phi(x_i)]^2\\\\& \\mbox{subject to} \\sum_{j=1}^D |w_j|^q\\leq \\eta\\end{aligned}$$\n",
    "\n",
    "式中，$\\eta$是某个大于$0$的常数  \n",
    "\n",
    "下图为我们演示了上述不等式优化问题在二维的情况，图中红线包裹的区域是$\\mathbf{w}$需要满足的不等式约束，蓝色的同心圆表示目标函数的等高线（越往里，函数值越小）\n",
    "\n",
    "<img src=\"http://7xikew.com1.z0.glb.clouddn.com/PRML-3.1-3.png\" width=600px>\n",
    "\n",
    "经过观察，可以知道，$q=1$时，Lasso的L1约束导致了$\\mathbf{w}_1^*=0$。当$\\lambda$越大时，这种约束将会越明显，权值向量将变得越稀疏。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用SVD实现ridge regression\n",
    "\n",
    "今天在实现岭回归的时候发现了一个问题，我写好了如下的岭回归代码，接着我将正则化系数设为0（退化为线性回归），输出拟合曲线的系数，然后我又将其与线性回归的系数输出进行比较，发现两者不一致且相差很大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights of ridge regression(lambda=0):\n",
      "w_0=-0.17, w_1=19.90, w_2=-141.49, w_3=485.66, w_4=-503.59, w_5=-1146.10, w_6=2537.15, w_7=-79.13, w_8=-2483.43, w_9=1311.17, \n",
      "weights of linear regression:\n",
      "w_0=-0.24, w_1=73.92, w_2=-1227.70, w_3=9413.51, w_4=-40494.25, w_5=106457.07, w_6=-175722.05, w_7=177370.64, w_8=-99533.63, w_9=23662.71, \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def polyfit(x, t, M):\n",
    "    '''polynomial curve fitting\n",
    "    # Arguments:\n",
    "        x: vector of input variables\n",
    "        t: targets of input variables\n",
    "        M: degree of polynomial model\n",
    "    # Returns:\n",
    "        coefficients of the polynomial model\n",
    "    '''\n",
    "    X = np.array([[xx ** m for m in range(M+1)] for xx in x], dtype='float32')\n",
    "    #return np.linalg.pinv(X).dot(t)\n",
    "    # more accurate version, equivalent to pinv\n",
    "    return np.linalg.lstsq(X, t)[0]\n",
    "\n",
    "M = 9\n",
    "_lambda = 0\n",
    "x = np.linspace(0, 1, 501)\n",
    "t = np.sin(2 * np.pi * x)\n",
    "# 生成0~1之间等间距的10个点\n",
    "n = 10\n",
    "x_tr = np.linspace(0, 1, n)\n",
    "t_tr = np.sin(2 * np.pi * x_tr) + np.random.normal(0, 0.3, n)\n",
    "Phi = np.array([[xx ** m for m in range(M+1)] for xx in x_tr], dtype='float32')    \n",
    "w_ridge = np.linalg.solve(Phi.T.dot(Phi)+_lambda * np.eye(M+1), Phi.T.dot(t_tr))\n",
    "wgt_info = ''\n",
    "print 'weights of ridge regression(lambda={}):'.format(_lambda)\n",
    "for i, ww in enumerate(w_ridge):\n",
    "    wgt_info += 'w_{}={:.2f}, '.format(i, ww)\n",
    "print wgt_info\n",
    "\n",
    "w_lr = polyfit(x_tr, t_tr, M)\n",
    "wgt_info = ''\n",
    "print 'weights of linear regression:'\n",
    "for i, ww in enumerate(w_lr):\n",
    "    wgt_info += 'w_{}={:.2f}, '.format(i, ww)\n",
    "print wgt_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于这个问题我百思不得其解，于是求助于网络，经过一番查找，大概得到的答案是直接用solve函数求解在式子左边的矩阵接近奇异时会出现数值上的不稳定，解决方法是用奇异值分解过滤掉接近0的奇异值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾岭回归的求解公式：\n",
    "$$w^*=(X^\\top X+\\lambda I)^{-1}X^\\top y$$\n",
    "将矩阵$X$进行奇异值分解有：\n",
    "$$ X=U\\Sigma V^\\top$$\n",
    "我们要求解的是伪逆矩阵$X^\\dagger=(X^\\top X+\\lambda I)^{-1}X^\\top$，将分解式代入有：\n",
    "$$\\begin{aligned}(X^\\top X+\\lambda I)^{-1}X^\\top &=(V\\Sigma (U^\\top U)\\Sigma V^\\top+\\lambda I)^{-1}\\cdot V\\Sigma U^\\top\\\\&=(V\\Sigma^2 V^T+\\lambda V V^\\top)^{-1}\\cdot V\\Sigma U^\\top\\\\&=\\big(V(\\Sigma^2 +\\lambda I) V^\\top\\big)^{-1}\\cdot V\\Sigma U^\\top\\\\&=(V^\\top)^{-1}\\cdot(\\Sigma^2+\\lambda I)^{-1}\\cdot(V)^{-1}\\cdot V\\Sigma U^\\top\\\\&=V(\\Sigma^2+\\lambda I)^{-1}\\Sigma U^\\top\\\\&=V \\cdot diag(\\frac{\\sigma_1}{\\sigma_1^2+\\lambda},...,\\frac{\\sigma_k}{\\sigma_k^2+\\lambda})\\cdot U^\\top\\end{aligned}$$\n",
    "实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_regression(x, t,_lambda, M):\n",
    "    '''ridge regression\n",
    "    # Arguments:\n",
    "        x: vector of input variables\n",
    "        t: targets of input variables\n",
    "        M: degree of polynomial model\n",
    "    # Returns:\n",
    "        coefficients of the polynomial model    \n",
    "    '''\n",
    "    Phi = np.array([[xx ** m for m in range(M+1)] for xx in x], dtype='float32')\n",
    "    U, S, Vh = np.linalg.svd(Phi, full_matrices=False)\n",
    "    Ut = U.T.dot(t)\n",
    "    #return np.linalg.solve(Phi.T.dot(Phi) + _lambda * np.eye(M+1)), Phi.T.dot(t))\n",
    "    return reduce(np.dot, [Vh.T, np.diag(S/(S**2 + _lambda)), Ut])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights of ridge regression(lambda=0):\n",
      "w_0=-0.24, w_1=73.92, w_2=-1227.70, w_3=9413.51, w_4=-40494.27, w_5=106457.13, w_6=-175722.14, w_7=177370.73, w_8=-99533.68, w_9=23662.72, \n",
      "weights of linear regression:\n",
      "w_0=-0.24, w_1=73.92, w_2=-1227.70, w_3=9413.51, w_4=-40494.25, w_5=106457.07, w_6=-175722.05, w_7=177370.64, w_8=-99533.63, w_9=23662.71, \n"
     ]
    }
   ],
   "source": [
    "M = 9\n",
    "_lambda = 0\n",
    "w_ridge = ridge_regression(x_tr, t_tr, _lambda, M)\n",
    "wgt_info = ''\n",
    "print 'weights of ridge regression(lambda={}):'.format(_lambda)\n",
    "for i, ww in enumerate(w_ridge):\n",
    "    wgt_info += 'w_{}={:.2f}, '.format(i, ww)\n",
    "print wgt_info\n",
    "\n",
    "w_lr = polyfit(x_tr, t_tr, M)\n",
    "wgt_info = ''\n",
    "print 'weights of linear regression:'\n",
    "for i, ww in enumerate(w_lr):\n",
    "    wgt_info += 'w_{}={:.2f}, '.format(i, ww)\n",
    "print wgt_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，当正则系数等于0时，svd版本的岭回归的结果和线性回归的结果基本一致。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

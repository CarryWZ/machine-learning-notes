{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用numpy实现RNN\n",
    "本文针对上一个笔记[BPTT算法数学推导](./BackPropagation Through Time.ipynb)，用numpy实现了一个RNN，部分代码参考自wildML，在其基础上进行了包装，并融入了自己的一些理解，力求写的通俗易懂。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取语料库\n",
    "首先是读取语料库的函数，这里直接摘抄自wildML的代码，这段代码写的十分精巧，将许多功能浓缩在一行，实在是令我叹为观止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "读取语料库\n",
    "'''\n",
    "def load_corpus():\n",
    "    print '='*50\n",
    "    print 'Loading CSV file...'\n",
    "    with open('data/reddit-comments-2015-08.csv', 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        # 跳过开头的'body'\n",
    "        reader.next()\n",
    "        # 将整篇文档分成句子的列表\n",
    "        sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "        # 为每句话加上 SENTENCE_START 和 SENTENCE_END 符号\n",
    "        sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "    print 'Parsed %d sentences.'%len(sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将文档拆成句子列表的这一句话写的非常geek，也非常难懂\n",
    "```\n",
    "sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "```\n",
    "我来分析一下这句话干了哪些事情：\n",
    "1. 调用nltk的`sent_tokenize`对段落x[0]分句，将其转变为句子的列表，x[0]表示csv文件里一行的内容\n",
    "2. 用列表生成式遍历csv文件每一行，生成句子列表的嵌套列表\n",
    "3. 用星号`*`对这个嵌套列表解包，将其变为一个一个的列表\n",
    "4. 用`chain()`函数将这些列表合并为一个列表  \n",
    "\n",
    "`chain()`函数的定义如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chain(*iterables):\n",
    "    # chain('ABC', 'DEF') --> A B C D E F\n",
    "    for it in iterables:\n",
    "        for element in it:\n",
    "            yield element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它的作用是将多个迭代器作为参数, 但只返回单个迭代器, 它产生所有参数迭代器的内容, 就好像他们是来自于一个单一的序列。  \n",
    "最后再用一个列表生成式为每一句话加上起始符和结束符。\n",
    "```\n",
    "sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本的预处理\n",
    "文本预处理主要完成了：\n",
    "1. 对句子的分词\n",
    "2. 统计词频\n",
    "3. 获取高频词，建立索引\n",
    "4. 替换未登录词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "文本预处理\n",
    "'''\n",
    "def preprocessing(sentences):\n",
    "    print '='*50\n",
    "    print 'Preprocessing...'\n",
    "    # 对每个句子进行分词\n",
    "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    # 统计词频\n",
    "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
    "    # 获得词频最高的词，并建立索引到词、词到索引的向量\n",
    "    vocab = word_freq.most_common(vocabulary_size-1)\n",
    "    index_to_word = [x[0] for x in vocab]\n",
    "    index_to_word.append(unknown_token)\n",
    "    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "    print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "    print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])                 \n",
    "    # 将未登录词替换为unknown token\n",
    "    for i, sent in enumerate(tokenized_sentences):\n",
    "        tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "    return tokenized_sentences, word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对句子分词\n",
    "`nltk.word_tokenize(sent)`作用是对输入的句子进行分词，输出为分词结果组成的列表。`tokenized_sentences`是一个嵌套列表，列表中的每一个元素表示一条句子的分词结果。\n",
    "2. 统计词频\n",
    "首先用`*`将`tokenized_sentences`分解为多个list，接着调用`itertools.chain()`函数将这些list接合成一个list，作为`nltk.FreqDist()`函数的输入，返回一个字典，键为word，值为该word的词频。\n",
    "3. 获取高频词，建立索引\n",
    "用`most_common`函数获取频率最高的`vocabulary_size-1`个词，返回值`vocab`是一个tuple的list，其格式如下：\n",
    "```\n",
    "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024),\n",
    "('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)]\n",
    "```\n",
    "接着我们遍历这个list，将高频词从每个tuple取出来单独构成一个list，并在最后加入unknown_token表示未登录词：\n",
    "```\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "```\n",
    "然后我们要建立词的索引，`enumerate()`函数接收一个list，并为list中的每个元素生成一个序号。一种较为naive的写法是"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for item in iterable:\n",
    "    print i, item\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而使用enumerate我们可以将代码简化为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, item in enumerate(iterable):\n",
    "    print i, item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们生成一个由词和索引构成的tuple组成的list，并用它生成一个dict `word_to_index`。\n",
    "\n",
    "4.替换未登录词\n",
    "这里使用了一种较为高级的列表生成式：\n",
    "```\n",
    "[w if w in word_to_index else unknown_token for w in sent]\n",
    "```\n",
    "在生成列表时候，我们可以加入条件判断以完成复杂的表达式赋值，这里如果w出现在索引字典中则保持不变，否则将被替换为unknown_token。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成训练数据\n",
    "上一步中，我们获得了分词结果`tokenized_sentences`和词索引`word_to_index`，这一节我们利用它们来生成训练集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "生成训练数据\n",
    "'''\n",
    "def gen_train_data():\n",
    "    # 读取语料库\n",
    "    sentences = load_corpus()\n",
    "    # 预处理\n",
    "    tokenized_sentences, word_to_index = preprocessing(sentences)\n",
    "    # 创建训练数据\n",
    "    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gen_train_data()`函数首先读取语料库，接着对句子进行了预处理，最后生成训练数据集。`X_train`和`y_train`都是二维numpy数组，在初始化这两个数组时我们采用了含有两层嵌套循环的列表生成式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一层循环遍历每一条句子的分词序列，第二层循环遍历序列中的每个词将它们转化为索引。\n",
    "注意到`sent[:-1]`代表`[sentence_start_token x ]`，`sent[1:]`代表`[x sentence_end_token]`，这是由于我们这个任务是训练一个语言模型，每次预测下一个出现的词，因此`y_train`中每个词对应`X_train`中下一个出现的词，即$y_t=x_{t+1}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现softmax函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解释一下为什么要减去max(x)。由于x可能会很大，exp以后可能会上溢出（overflow），而softmax是一个比值，因此如果我们在分子分母上同时乘/除以一个常数是不会影响最终结果的，我们只要同时除以一个较大的数就可以防止overflow。显然这里除以最大值是最好的，因为我们无法确定x的数量级，而最大值是关于x数量级自适应增长的。 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

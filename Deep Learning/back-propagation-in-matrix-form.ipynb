{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BackPropagation算法的矩阵形式推导\n",
    "\n",
    "## 符号说明\n",
    "|符号|说明|\n",
    "|:|:|\n",
    "|$d^{(l)}$|第$l$层的节点个数|\n",
    "| $W^{(l)}$|$d^{(l)}\\times d^{(l-1)}$的权值矩阵，$W^{(l)}_{ij}$表示第$l-1$层节点$j$到第$l$层节点$i$的突触权值|\n",
    "| $x^{(l)}$|第$l$层每个节点输出组成的向量，是一个$d^{(l)}\\times 1$的列向量|\n",
    "|$s^{(l)}$|第$l$层每个节点输入组成的向量，同上|\n",
    "|$b^{(l)}$|第$l$层每个节点的偏置，同上|\n",
    "| $\\sigma(x)$|_sigmoid_函数，$\\sigma(x)=1/(1+exp(-x))$，它的导数是$\\sigma^\\prime(x)=\\sigma(x)(1-\\sigma(x))$|\n",
    "| $n$|样本数|\n",
    "| $J(W,b;x_i,y_i)$|当输入为$(x_i,y_i)$时的损失函数，以下简记为$J$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前馈过程和目标函数\n",
    "神经网络的前馈公式：\n",
    "$$ \\left\\{ \\begin{aligned} & s^{(l)} = W^{(l)} \\cdot x^{(l-1)}  + b^{(l)}\\\\ & x^{(l)} = \\sigma(s^{(l)})  \\end{aligned} \\right. $$\n",
    "\n",
    "神经网络的目标是寻找使得如下目标函数最大的参数：\n",
    "$$ J(\\theta) =  \\frac{1}{n} \\sum_{i=1}^n \\left\\|y_i - \\hat{y}_i\\right\\|^2$$\n",
    "其中，$\\theta$是参数集合，$\\hat{y}_i=\\sigma(W^{(L)}x_i^{(L-1)}+ b^{(L)})$是神经网络的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BP算法推导\n",
    "BP算法的关键是计算$\\delta^{(l)}=\\frac{\\partial J}{\\partial s^{(l)}}$，其含义是最终误差对于第$l$层神经元的敏感程度。\n",
    "依据链式法则，有\n",
    "$$ \\begin{aligned}\\delta^{(l)}=\\frac{\\partial J}{\\partial s^{(l)}} = &( \\frac{\\partial J}{\\partial s^{(l+1)}} \\cdot \\frac{\\partial s^{(l+1)}}{\\partial x^{(l)}} \\cdot\\frac{\\partial x^{(l)}}{\\partial s^{(l)}} ) ^T\\\\ = &\\{ (\\delta^{(l+1)})^T \\cdot W^{(l+1)} \\cdot diag(\\sigma^\\prime(s^{(l)}))\\}^T\\\\=& diag(\\sigma^\\prime(s^{(l)}))\\cdot (W^{(l+1)})^T\\delta^{(l+1)}\\\\=& \\sigma^\\prime(s^{(l)})\\odot ((W^{(l+1)})^T\\delta^{(l+1)})\\end{aligned}$$\n",
    "其中$\\odot$是向量点积。于是我们有了误差敏感度的递推公式：\n",
    "$$ \\delta^{(l)} = \\sigma^\\prime(s^{(l)})\\odot ((W^{(l+1)})^T\\delta^{(l+1)})$$\n",
    "于是，梯度为\n",
    "$$ \\nabla{W}^{(l)}=\\frac{\\partial J}{\\partial W^{(l)}}=\\delta^{(l)}\\otimes x^{(l-1)}$$\n",
    "$$ W^{(l)}\\longleftarrow W^{(l)} - \\alpha  \\nabla{W}^{(l)}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
